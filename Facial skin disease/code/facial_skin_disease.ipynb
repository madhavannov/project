{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy tensorflow tensorflow-hub google-colab scikit-image matplotlib seaborn scikit-learn tqdm pillow\n"
      ],
      "metadata": {
        "id": "6lELR_dpHsZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39vbWvMIHkSg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from google.colab import drive\n",
        "from skimage import transform\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, f1_score, classification_report\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "# 1. Data Collection and Preprocessing\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path Variables\n",
        "train_path = '/content/drive/My Drive/Facial Skin Disease Dataset'\n",
        "\n",
        "# Define Some Variables\n",
        "batch_size = 32\n",
        "img_height = 224  # ResNet50V2 standard input size\n",
        "img_width = 224   # ResNet50V2 standard input size\n",
        "seed_train_validation = 1\n",
        "shuffle_value = True\n",
        "validation_split = 0.2\n",
        "\n",
        "# Load Training Images without Augmentation\n",
        "initial_train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  train_path,\n",
        "  validation_split=validation_split,\n",
        "  subset=\"training\",\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  seed=seed_train_validation,\n",
        "  shuffle=shuffle_value\n",
        ")\n",
        "\n",
        "# Extract the class names\n",
        "class_names = initial_train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(\"The target classes are:\", *class_names, sep=\", \")\n",
        "\n",
        "# Data Augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip('horizontal_and_vertical'),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomContrast(0.2)\n",
        "])\n",
        "\n",
        "# Load Training Images with Augmentation\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  train_path,\n",
        "  validation_split=validation_split,\n",
        "  subset=\"training\",\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  seed=seed_train_validation,\n",
        "  shuffle=shuffle_value\n",
        ")\n",
        "\n",
        "# Load Validation Images\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  train_path,\n",
        "  validation_split=validation_split,\n",
        "  subset=\"validation\",\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size,\n",
        "  seed=seed_train_validation,\n",
        "  shuffle=shuffle_value\n",
        ")\n",
        "\n",
        "# Rescaling the Images for the Model\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# Apply Data Augmentation\n",
        "def augment_image(image, label):\n",
        "    image = data_augmentation(image)\n",
        "    return image, label\n",
        "\n",
        "train_ds = train_ds.map(augment_image)\n",
        "\n",
        "# Finish the Input Pipeline by Using Buffered Prefetching with Dataset.prefetch\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# 2. Model Development\n",
        "\n",
        "# Load Pre-trained ResNet50V2 from TensorFlow Hub\n",
        "resnet50_v2_fv = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\"  # Pre-trained ResNet50V2\n",
        "\n",
        "# Feature Extractor Layer (ResNet50V2)\n",
        "feature_extractor_layer = hub.KerasLayer(\n",
        "    resnet50_v2_fv,\n",
        "    input_shape=(img_width, img_height, 3),\n",
        "    trainable=False  # Freeze the pre-trained weights\n",
        ")\n",
        "\n",
        "# Build the Model\n",
        "model = tf.keras.Sequential([\n",
        "    feature_extractor_layer,\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
        "])\n",
        "\n",
        "# Model Summary\n",
        "model.summary()\n",
        "\n",
        "# Compile the Model\n",
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 3. Model Training\n",
        "\n",
        "# Early Stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "# Define Epochs\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    callbacks=[early_stopping, TqdmCallback(verbose=0)],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 4. Model Evaluation\n",
        "\n",
        "# Predictions\n",
        "val_labels = np.concatenate([y for x, y in val_ds], axis=0)\n",
        "val_preds = np.concatenate([model.predict(x) for x, y in val_ds], axis=0)\n",
        "val_preds_class = np.argmax(val_preds, axis=1)\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_mat = confusion_matrix(val_labels, val_preds_class)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "val_labels_bin = label_binarize(val_labels, classes=range(num_classes))\n",
        "val_preds_bin = tf.nn.softmax(val_preds).numpy()\n",
        "\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(num_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(val_labels_bin[:, i], val_preds_bin[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(num_classes):\n",
        "    plt.plot(fpr[i], tpr[i], lw=2, label=f'ROC curve of class {class_names[i]} (area = {roc_auc[i]:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# F1 Score\n",
        "f1 = f1_score(val_labels, val_preds_class, average='weighted')\n",
        "print(f'Weighted F1 Score: {f1:.2f}')\n",
        "\n",
        "# Classification Report\n",
        "class_report = classification_report(val_labels, val_preds_class, target_names=class_names)\n",
        "print('Classification Report:\\n', class_report)\n",
        "\n",
        "# 5. Model Deployment\n",
        "\n",
        "# Save the Model\n",
        "model.save('/content/drive/My Drive/save model/facial_skin_disease_resnet50_v2.h5')\n",
        "\n",
        "# Load the Model\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/My Drive/save model/facial_skin_disease_resnet50_v2.h5', custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "\n",
        "# Prediction on New Images\n",
        "from google.colab import files\n",
        "\n",
        "# Upload Image\n",
        "uploaded = files.upload()\n",
        "uploaded_file_name = list(uploaded.keys())[0]\n",
        "test_img_path = uploaded_file_name\n",
        "\n",
        "# Resize Image\n",
        "test_image = Image.open(test_img_path)\n",
        "test_image = np.array(test_image).astype('float32') / 255\n",
        "test_image = transform.resize(test_image, (img_width, img_height, 3))\n",
        "test_image = np.expand_dims(test_image, axis=0)\n",
        "\n",
        "# Make Predictions\n",
        "prediction = loaded_model.predict(test_image)\n",
        "pred_class = np.argmax(prediction, axis=1)[0]\n",
        "print(f\"The Predicted Class: {class_names[pred_class]}\\n\")\n",
        "\n",
        "# View the Uploaded Image\n",
        "plt.figure(figsize=(3, 3))\n",
        "test_img = mpimg.imread(test_img_path)\n",
        "plt.imshow(test_img)\n",
        "plt.title(\"Predicted Class: \" + class_names[pred_class])\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ]
}